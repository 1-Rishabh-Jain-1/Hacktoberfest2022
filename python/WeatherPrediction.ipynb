import warnings
warnings.filterwarnings("ignore")
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
features=pd.read_csv('temps.csv')
features.head()

[2]
features.shape
(348, 12)
[3]
features.describe()

[4]
data=features.iloc[:,4:]
[5]
fig, ax = plt.subplots(figsize =(10, 7))
ax.hist(data)
(array([[  3.,  25.,  65.,  90.,  85.,  62.,  16.,   1.,   0.,   1.],
        [  3.,  24.,  64.,  92.,  85.,  62.,  16.,   1.,   0.,   1.],
        [  0.,  23., 117.,  73.,  76.,  59.,   0.,   0.,   0.,   0.],
        [  3.,  24.,  64.,  92.,  86.,  62.,  16.,   1.,   0.,   0.],
        [  0.,  58., 104.,  71.,  77.,  38.,   0.,   0.,   0.,   0.],
        [  0.,   0., 111.,  80.,  75.,  81.,   1.,   0.,   0.,   0.],
        [  0.,  18., 116.,  78.,  75.,  61.,   0.,   0.,   0.,   0.],
        [ 22.,  52.,  50.,  81.,  64.,  44.,  28.,   7.,   0.,   0.]]),
 array([ 28. ,  36.9,  45.8,  54.7,  63.6,  72.5,  81.4,  90.3,  99.2,
        108.1, 117. ]),
 <a list of 8 BarContainer objects>)

[6]
data.hist(bins=30, figsize=(15, 10));

[7]
features=pd.get_dummies(features)
features.iloc[:,5:].head(30)

[8]
labels = np.array(features['actual'])
features= features.drop('actual', axis = 1)
features_list=list(features.columns)
features = np.array(features)
[24]
print(features_list) #[]
['year', 'month', 'day', 'temp_2', 'temp_1', 'average', 'forecast_noaa', 'forecast_acc', 'forecast_under', 'friend', 'week_Fri', 'week_Mon', 'week_Sat', 'week_Sun', 'week_Thurs', 'week_Tues', 'week_Wed']

[9]
from sklearn.model_selection import train_test_split
train_features, test_features, train_labels, test_labels = train_test_split(features,labels, test_size = 0.25, random_state = 42)
[10]
baseline_preds=test_features[:,features_list.index('average')]
baseline_errors=abs(baseline_preds-test_labels)
print("Baseline: ",np.mean(baseline_errors))
Baseline:  5.05977011494253

[11]
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)
rf.fit(train_features, train_labels)
RandomForestRegressor(n_estimators=1000, random_state=42)
[12]
predictions = rf.predict(test_features)
errors = abs(predictions - test_labels)
print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')
Mean Absolute Error: 3.87 degrees.

[13]
mape = 100 * (errors / test_labels)
accuracy = 100 - np.mean(mape)
print('Accuracy:', round(accuracy, 2), '%.')
Accuracy: 93.93 %.

[14]
from sklearn.tree import export_graphviz
import pydot
# Pull out one tree from the forest
tree = rf.estimators_[5]
# Export the image to a dot file
export_graphviz(tree, out_file = 'tree.dot', feature_names = features_list, rounded = True, precision = 1)
# Use dot file to create a graph
(graph, ) = pydot.graph_from_dot_file('tree.dot')
# Write graph to a png file
graph.write_png('tree.png')
[15]
rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3)
rf_small.fit(train_features, train_labels)
tree_small = rf_small.estimators_[5]
export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = features_list, rounded = True, precision = 1)
(graph, ) = pydot.graph_from_dot_file('small_tree.dot')
graph.write_png('small_tree.png');
[16]
importances = list(rf.feature_importances_)
# List of tuples with variable and importance
feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(features_list, importances)]
# Sort the feature importances by most important first
feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)
# Print out the feature and importances 
[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];
Variable: temp_1               Importance: 0.66
Variable: average              Importance: 0.15
Variable: forecast_noaa        Importance: 0.05
Variable: forecast_acc         Importance: 0.03
Variable: day                  Importance: 0.02
Variable: temp_2               Importance: 0.02
Variable: forecast_under       Importance: 0.02
Variable: friend               Importance: 0.02
Variable: month                Importance: 0.01
Variable: year                 Importance: 0.0
Variable: week_Fri             Importance: 0.0
Variable: week_Mon             Importance: 0.0
Variable: week_Sat             Importance: 0.0
Variable: week_Sun             Importance: 0.0
Variable: week_Thurs           Importance: 0.0
Variable: week_Tues            Importance: 0.0
Variable: week_Wed             Importance: 0.0

[17]
x_values = list(range(len(importances)))
# Make a bar chart
plt.bar(x_values, importances, orientation = 'vertical')
# Tick labels for x axis
plt.xticks(x_values, features_list, rotation='vertical')
# Axis labels and title
plt.ylabel('Importance'); 
plt.xlabel('Variable'); 
plt.title('Variable Importances');

[18]
import datetime
months = features[:, features_list.index('month')]
days = features[:, features_list.index('day')]
years = features[:, features_list.index('year')]
# List and then convert to datetime object
dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]
dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]
# Dataframe with true values and dates
true_data = pd.DataFrame(data = {'date': dates, 'actual': labels})
# Dates of predictions
months = test_features[:, features_list.index('month')]
days = test_features[:, features_list.index('day')]
years = test_features[:, features_list.index('year')]
# Column of dates
test_dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]
# Convert to datetime objects
test_dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in test_dates]
# Dataframe with predictions and dates
predictions_data = pd.DataFrame(data = {'date': test_dates, 'prediction': predictions})
# Plot the actual values
plt.plot(true_data['date'], true_data['actual'], 'b-', label = 'actual')
# Plot the predicted values
plt.plot(predictions_data['date'], predictions_data['prediction'], 'ro', label = 'prediction')
plt.xticks(rotation = '60'); 
plt.legend()
# Graph labels
plt.xlabel('Date');
plt.ylabel('Maximum Temperature (F)');
plt.title('Actual and Predicted Values');

Grid Search
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier

param_grid = [{ 
    'n_estimators': [200,500, 700,1000,1200,1500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'min_samples_leaf':[2,4,6],
    'min_samples_split':[2,4,6],
    'oob_score' : ['TRUE'],
    'random_state':[1,42],
    'max_depth' : [3,4,5,6,7,8],
    
    
}]

CV_rfc = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 5)
CV_rfc.fit(train_features, train_labels)
CV_rfc.best_params_
[21]
rfc1=RandomForestRegressor(random_state=1, max_features='sqrt', n_estimators= 200, max_depth=8, criterion='entropy')
[22]
rfc1.fit(train_features, train_labels)
predictions = rfc1.predict(test_features)
errors = abs(predictions - test_labels)
print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')
Mean Absolute Error: 4.75 degrees.

[23]
mape = 100 * (errors / test_labels)
accuracy = 100 - np.mean(mape)
print('Accuracy:', round(accuracy, 2), '%.')
